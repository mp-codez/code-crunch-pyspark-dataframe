# Extract the first three rows
header_rows = df.limit(3).collect()

# Combine the rows into column headers
new_columns = ["_".join([str(header_rows[j][i]) for j in range(3)]) for i in range(len(header_rows[0]))]

# Select the remaining rows and assign the new columns
# Skip the first 3 rows using filter with row_number
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Adding a row number column to identify and skip the first 3 rows
df_with_row_num = df.withColumn("row_num", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))

# Filtering out the first 3 rows
df_filtered = df_with_row_num.filter(F.col("row_num") > 3).drop("row_num")

# Applying new column names to the remaining DataFrame
for i, new_col in enumerate(new_columns):
    df_filtered = df_filtered.withColumnRenamed(df_filtered.columns[i], new_col)

# Display the DataFrame
df_filtered.show()


≈=============/===============
# Step 1: Extract the first 2 rows to use as header rows
header_rows = df.limit(2).collect()

# Step 2: Set up new column names combining both rows
column_names = [f"{str(header_rows[0][i])} ({str(header_rows[1][i])})" for i in range(len(header_rows[0]))]

# Step 3: Skip the first 2 rows and keep the remaining data
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Add row number to identify and filter out the first 2 rows
df_with_row_num = df.withColumn("row_num", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))

# Filter out the first 2 rows, keeping only the data rows
df_filtered = df_with_row_num.filter(F.col("row_num") > 2).drop("row_num")

# Step 4: Rename columns with the new headers
df_final = df_filtered.toDF(*column_names)

# Display the final DataFrame
df_final.show()


№###№@###########
from pyspark.sql.types import StructType, StructField, StringType
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Step 1: Extract the first 2 rows
header_rows = df.limit(2).collect()

# Step 2: Define column names and units from the extracted rows
column_names = [str(header_rows[0][i]) for i in range(len(header_rows[0]))]
unit_types = [str(header_rows[1][i]) for i in range(len(header_rows[1]))]

# Step 3: Create StructType schema, leaving units blank for the first 5 columns
schema = StructType([
    StructField(f"{col_name}" if i < 5 else f"{col_name} ({unit})", StringType(), True)
    for i, (col_name, unit) in enumerate(zip(column_names, unit_types))
])

# Step 4: Skip the first 2 rows and apply the new schema
df_with_row_num = df.withColumn("row_num", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))
df_filtered = df_with_row_num.filter(F.col("row_num") > 2).drop("row_num")

# Apply the schema
df_final = spark.createDataFrame(df_filtered.rdd, schema)

# Display the final DataFrame
df_final.show()