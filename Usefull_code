# Extract the first three rows
header_rows = df.limit(3).collect()

# Combine the rows into column headers
new_columns = ["_".join([str(header_rows[j][i]) for j in range(3)]) for i in range(len(header_rows[0]))]

# Select the remaining rows and assign the new columns
# Skip the first 3 rows using filter with row_number
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Adding a row number column to identify and skip the first 3 rows
df_with_row_num = df.withColumn("row_num", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))

# Filtering out the first 3 rows
df_filtered = df_with_row_num.filter(F.col("row_num") > 3).drop("row_num")

# Applying new column names to the remaining DataFrame
for i, new_col in enumerate(new_columns):
    df_filtered = df_filtered.withColumnRenamed(df_filtered.columns[i], new_col)

# Display the DataFrame
df_filtered.show()


≈=============/===============
# Step 1: Extract the first 2 rows to use as header rows
header_rows = df.limit(2).collect()

# Step 2: Set up new column names combining both rows
column_names = [f"{str(header_rows[0][i])} ({str(header_rows[1][i])})" for i in range(len(header_rows[0]))]

# Step 3: Skip the first 2 rows and keep the remaining data
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Add row number to identify and filter out the first 2 rows
df_with_row_num = df.withColumn("row_num", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))

# Filter out the first 2 rows, keeping only the data rows
df_filtered = df_with_row_num.filter(F.col("row_num") > 2).drop("row_num")

# Step 4: Rename columns with the new headers
df_final = df_filtered.toDF(*column_names)

# Display the final DataFrame
df_final.show()


№###№@###########
from pyspark.sql.types import StructType, StructField, StringType
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Step 1: Extract the first 2 rows
header_rows = df.limit(2).collect()

# Step 2: Define column names and units from the extracted rows
column_names = [str(header_rows[0][i]) for i in range(len(header_rows[0]))]
unit_types = [str(header_rows[1][i]) for i in range(len(header_rows[1]))]

# Step 3: Create StructType schema, leaving units blank for the first 5 columns
schema = StructType([
    StructField(f"{col_name}" if i < 5 else f"{col_name} ({unit})", StringType(), True)
    for i, (col_name, unit) in enumerate(zip(column_names, unit_types))
])

# Step 4: Skip the first 2 rows and apply the new schema
df_with_row_num = df.withColumn("row_num", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))
df_filtered = df_with_row_num.filter(F.col("row_num") > 2).drop("row_num")

# Apply the schema
df_final = spark.createDataFrame(df_filtered.rdd, schema)

# Display the final DataFrame
df_final.show()



№########
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Initialize Spark session
spark = SparkSession.builder.appName("PartialPressureCalculation").getOrCreate()

def partial_pressure(df_s, df_xy, lst):
    # Select only the relevant columns
    df_mini = df_xy.select("LibraryID", "ExperimentID", "Position", "Pre-InjectPressure")

    # Get the first and last occurrences by grouping with a window function
    window_spec = Window.partitionBy("LibraryID", "ExperimentID", "Position").orderBy("some_order_column")  # Replace 'some_order_column' with a timestamp or other order indicator if available.

    # First occurrence
    first_occurrences = df_mini.withColumn("first_occurrence", F.first("Pre-InjectPressure").over(window_spec))

    # Last occurrence
    last_occurrences = df_mini.withColumn("last_occurrence", F.last("Pre-InjectPressure").over(window_spec))

    # Calculate the difference between the first and last occurrences
    result_df = first_occurrences.join(last_occurrences, on=["LibraryID", "ExperimentID", "Position"])
    result_df = result_df.withColumn(lst[92], F.col("last_occurrence") - F.col("first_occurrence"))

    # Remove duplicates, keeping only the last occurrence of each group
    result_df = result_df.dropDuplicates(["LibraryID", "ExperimentID", "Position"])

    # Sort the DataFrame by specified columns
    result_df = result_df.orderBy("LibraryID", "ExperimentID", "Position")

    # Create a unique identifier by concatenating columns
    result_df = result_df.withColumn("Unique Experiment ID (PPR = LibID+ExpID)", 
                                     F.concat_ws("-", F.col("LibraryID"), F.col("ExperimentID")))

    # Drop unnecessary columns
    result_df = result_df.drop("LibraryID", "ExperimentID", "Pre-InjectPressure")

    # Assuming df_s has the relevant columns to merge with, we join it on the unique experiment ID
    merged_df = df_s.join(result_df, df_s["Unique Experiment ID (PPR = LibID+ExpID)"] == result_df["Unique Experiment ID (PPR = LibID+ExpID)"], "left")

    # Drop the 'Position' column from the final DataFrame, if needed
    merged_df = merged_df.drop("Position")

    return merged_df

# Use the function with your DataFrames
# result_df = partial_pressure(df_s, df_xy, lst)


______<&><<<<<<<
from pyspark.sql.functions import col, sum as spark_sum, when
from pyspark.sql import DataFrame

# Assuming liquid_columns is a list of column names
liquid_columns = ['col1', 'col2', 'col3']  # Replace with your actual column names

# Convert columns to numeric, handle errors by replacing invalid values with null, and sum them
df_s = df_s.select(
    *df_s.columns,
    spark_sum(
        *[when(col(c).cast('double').isNotNull(), col(c).cast('double')).otherwise(0) for c in liquid_columns]
    ).alias('sum_column')
)

df_s.show()