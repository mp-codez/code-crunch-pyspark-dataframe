# Extract the first three rows
header_rows = df.limit(3).collect()

# Combine the rows into column headers
new_columns = ["_".join([str(header_rows[j][i]) for j in range(3)]) for i in range(len(header_rows[0]))]

# Select the remaining rows and assign the new columns
# Skip the first 3 rows using filter with row_number
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Adding a row number column to identify and skip the first 3 rows
df_with_row_num = df.withColumn("row_num", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))

# Filtering out the first 3 rows
df_filtered = df_with_row_num.filter(F.col("row_num") > 3).drop("row_num")

# Applying new column names to the remaining DataFrame
for i, new_col in enumerate(new_columns):
    df_filtered = df_filtered.withColumnRenamed(df_filtered.columns[i], new_col)

# Display the DataFrame
df_filtered.show()


≈=============/===============
# Step 1: Extract the first 2 rows to use as header rows
header_rows = df.limit(2).collect()

# Step 2: Set up new column names combining both rows
column_names = [f"{str(header_rows[0][i])} ({str(header_rows[1][i])})" for i in range(len(header_rows[0]))]

# Step 3: Skip the first 2 rows and keep the remaining data
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Add row number to identify and filter out the first 2 rows
df_with_row_num = df.withColumn("row_num", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))

# Filter out the first 2 rows, keeping only the data rows
df_filtered = df_with_row_num.filter(F.col("row_num") > 2).drop("row_num")

# Step 4: Rename columns with the new headers
df_final = df_filtered.toDF(*column_names)

# Display the final DataFrame
df_final.show()


№###№@###########
from pyspark.sql.types import StructType, StructField, StringType
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Step 1: Extract the first 2 rows
header_rows = df.limit(2).collect()

# Step 2: Define column names and units from the extracted rows
column_names = [str(header_rows[0][i]) for i in range(len(header_rows[0]))]
unit_types = [str(header_rows[1][i]) for i in range(len(header_rows[1]))]

# Step 3: Create StructType schema, leaving units blank for the first 5 columns
schema = StructType([
    StructField(f"{col_name}" if i < 5 else f"{col_name} ({unit})", StringType(), True)
    for i, (col_name, unit) in enumerate(zip(column_names, unit_types))
])

# Step 4: Skip the first 2 rows and apply the new schema
df_with_row_num = df.withColumn("row_num", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))
df_filtered = df_with_row_num.filter(F.col("row_num") > 2).drop("row_num")

# Apply the schema
df_final = spark.createDataFrame(df_filtered.rdd, schema)

# Display the final DataFrame
df_final.show()



№########
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Initialize Spark session
spark = SparkSession.builder.appName("PartialPressureCalculation").getOrCreate()

def partial_pressure(df_s, df_xy, lst):
    # Select only the relevant columns
    df_mini = df_xy.select("LibraryID", "ExperimentID", "Position", "Pre-InjectPressure")

    # Get the first and last occurrences by grouping with a window function
    window_spec = Window.partitionBy("LibraryID", "ExperimentID", "Position").orderBy("some_order_column")  # Replace 'some_order_column' with a timestamp or other order indicator if available.

    # First occurrence
    first_occurrences = df_mini.withColumn("first_occurrence", F.first("Pre-InjectPressure").over(window_spec))

    # Last occurrence
    last_occurrences = df_mini.withColumn("last_occurrence", F.last("Pre-InjectPressure").over(window_spec))

    # Calculate the difference between the first and last occurrences
    result_df = first_occurrences.join(last_occurrences, on=["LibraryID", "ExperimentID", "Position"])
    result_df = result_df.withColumn(lst[92], F.col("last_occurrence") - F.col("first_occurrence"))

    # Remove duplicates, keeping only the last occurrence of each group
    result_df = result_df.dropDuplicates(["LibraryID", "ExperimentID", "Position"])

    # Sort the DataFrame by specified columns
    result_df = result_df.orderBy("LibraryID", "ExperimentID", "Position")

    # Create a unique identifier by concatenating columns
    result_df = result_df.withColumn("Unique Experiment ID (PPR = LibID+ExpID)", 
                                     F.concat_ws("-", F.col("LibraryID"), F.col("ExperimentID")))

    # Drop unnecessary columns
    result_df = result_df.drop("LibraryID", "ExperimentID", "Pre-InjectPressure")

    # Assuming df_s has the relevant columns to merge with, we join it on the unique experiment ID
    merged_df = df_s.join(result_df, df_s["Unique Experiment ID (PPR = LibID+ExpID)"] == result_df["Unique Experiment ID (PPR = LibID+ExpID)"], "left")

    # Drop the 'Position' column from the final DataFrame, if needed
    merged_df = merged_df.drop("Position")

    return merged_df

# Use the function with your DataFrames
# result_df = partial_pressure(df_s, df_xy, lst)


______<&><<<<<<<
from pyspark.sql.functions import col, sum as spark_sum, when
from pyspark.sql import DataFrame

# Assuming liquid_columns is a list of column names
liquid_columns = ['col1', 'col2', 'col3']  # Replace with your actual column names

# Convert columns to numeric, handle errors by replacing invalid values with null, and sum them
df_s = df_s.select(
    *df_s.columns,
    spark_sum(
        *[when(col(c).cast('double').isNotNull(), col(c).cast('double')).otherwise(0) for c in liquid_columns]
    ).alias('sum_column')
)

df_s.show()



≈=====================
from pyspark.sql import SparkSession
from pyspark.sql.functions import coalesce

# Initialize Spark session
spark = SparkSession.builder.appName("Update Columns Example").getOrCreate()

# Sample DataFrames (Assume `df1` and `df2` are defined similarly in your setup)
data1 = [
    (1, "A", 100, "val1", "val2", "val3", "val4"),
    (2, "B", 200, "val5", "val6", "val7", "val8")
]
columns1 = ["col1", "col2", "col3", "fill_col1", "fill_col2", "fill_col3", "fill_col4"]
df1 = spark.createDataFrame(data1, columns1)

data2 = [
    (1, "A", 100, None, None, None, None),
    (2, "B", 200, None, None, None, None),
    (3, "C", 300, "existing1", "existing2", "existing3", "existing4")
]
columns2 = ["col1", "col2", "col3", "fill_col1", "fill_col2", "fill_col3", "fill_col4"]
df2 = spark.createDataFrame(data2, columns2)

# Perform a left join on the common columns
joined_df = df2.alias("df2").join(
    df1.alias("df1"),
    on=(df2["col1"] == df1["col1"]) & 
       (df2["col2"] == df1["col2"]) & 
       (df2["col3"] == df1["col3"]),
    how="left"
)

# Select all columns from df2, but update the target columns using coalesce
# Coalesce will pick values from df1 when they exist, otherwise df2 values are retained
updated_df2 = joined_df.select(
    df2["col1"], df2["col2"], df2["col3"],  # The common columns
    coalesce(df1["fill_col1"], df2["fill_col1"]).alias("fill_col1"),
    coalesce(df1["fill_col2"], df2["fill_col2"]).alias("fill_col2"),
    coalesce(df1["fill_col3"], df2["fill_col3"]).alias("fill_col3"),
    coalesce(df1["fill_col4"], df2["fill_col4"]).alias("fill_col4")
)

# Show the updated DataFrame
updated_df2.show()


≈======================================
import boto3
import math
from boto3.s3.transfer import TransferConfig

def multipart_upload(bucket_name, file_path, object_name):
    s3_client = boto3.client('s3')

    # Define the part size (e.g., 5 MB)
    part_size = 5 * 1024 * 1024

    # Get the file size
    file_size = os.path.getsize(file_path)

    # Create a transfer configuration
    config = TransferConfig(multipart_chunksize=part_size, use_threads=True)

    # Upload the file using multipart upload
    with open(file_path, 'rb') as file:
        s3_client.upload_fileobj(
            file,
            bucket_name,
            object_name,
            Config=config
        )
    print(f"{file_path} uploaded to S3 bucket {bucket_name} as {object_name}.")

# Example usage
multipart_upload(
    bucket_name='your-bucket-name',
    file_path='/path/to/large/file.txt',
    object_name='large-file.txt'
)

============≈==============//=========
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

def send_email(sender_email, receiver_email, subject, body, smtp_server, port, username=None, password=None):
    try:
        # Create a MIMEText email
        msg = MIMEMultipart()
        msg['From'] = sender_email
        msg['To'] = receiver_email
        msg['Subject'] = subject
        msg.attach(MIMEText(body, 'plain'))

        # Establish connection with the SMTP server
        with smtplib.SMTP(smtp_server, port) as server:
            server.starttls()  # Start TLS encryption
            if username and password:
                server.login(username, password)  # Authenticate with the server
            server.sendmail(sender_email, receiver_email, msg.as_string())  # Send the email
        
        print("Email sent successfully!")
    except Exception as e:
        print(f"Failed to send email: {e}")

# Example usage
send_email(
    sender_email="your_email@example.com",
    receiver_email="recipient_email@example.com",
    subject="Test Email",
    body="This is a test email sent from Python.",
    smtp_server="smtp.example.com",  # Replace with your SMTP server
    port=587,  # Use 465 for SSL or 587 for TLS
    username="your_email_username",  # Optional if server requires authentication
    password="your_email_password"   # Optional if server requires authentication
)


≈===============
import smtplib
import socket
import time
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

def is_server_up(host, port):
    """
    Check if the server is up by attempting a socket connection.
    """
    try:
        with socket.create_connection((host, port), timeout=5):
            return True
    except (socket.timeout, ConnectionError):
        return False

def send_email(sender_email, sender_password, recipient_email, subject, body):
    """
    Send an email notification using SMTP.
    """
    try:
        # Set up the email
        message = MIMEMultipart()
        message['From'] = sender_email
        message['To'] = recipient_email
        message['Subject'] = subject
        message.attach(MIMEText(body, 'plain'))
        
        # Connect to the SMTP server and send the email
        with smtplib.SMTP('smtp.gmail.com', 587) as server:
            server.starttls()
            server.login(sender_email, sender_password)
            server.send_message(message)
        print("Email sent successfully.")
    except Exception as e:
        print(f"Failed to send email: {e}")

def monitor_server(host, port, sender_email, sender_password, recipient_email):
    """
    Monitor server status every 5 minutes and send email if server is down.
    """
    while True:
        if not is_server_up(host, port):
            subject = f"Alert: Server {host} is Down"
            body = f"The server at {host} on port {port} is not responding."
            print(body)  # Log the issue
            send_email(sender_email, sender_password, recipient_email, subject, body)
        else:
            print(f"The server {host} is up and running.")  # Log the status
        time.sleep(300)  # Wait for 5 minutes before the next check

if __name__ == "__main__":
    # Server details
    host = "your_server_ip_or_hostname"
    port = 80  # Change to the port you want to monitor

    # Email details
    sender_email = "your_email@gmail.com"
    sender_password = "your_email_password"  # Use an app password if needed
    recipient_email = "recipient_email@gmail.com"

    monitor_server(host, port, sender_email, sender_password, recipient_email)



==========
from fastapi import FastAPI, Form, HTTPException
from pydantic import BaseModel
import os
from typing import Optional
import boto3
from boto3.s3.transfer import TransferConfig
from datetime import datetime
import uuid

app = FastAPI()

def upload_file_with_transfer_manager(file_path: str, bucket_name: str, client_id: Optional[str] = None):
    """
    Upload a file using TransferManager with a unique key for each client.
    """
    client = boto3.client(
        's3',
        endpoint_url='http://ozone-server:9878',  # Replace with your Ozone endpoint
        aws_access_key_id='your_access_key',
        aws_secret_access_key='your_secret_key'
    )

    unique_id = uuid.uuid4()
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    key_name = f"{client_id}_{timestamp}_{unique_id}_{os.path.basename(file_path)}" if client_id else f"{timestamp}_{unique_id}_{os.path.basename(file_path)}"

    config = TransferConfig(
        multipart_threshold=5 * 1024 * 1024,
        multipart_chunksize=5 * 1024 * 1024,
        use_threads=True
    )

    try:
        client.upload_file(
            Filename=file_path,
            Bucket=bucket_name,
            Key=key_name,
            Config=config
        )
        return f"File {file_path} uploaded successfully as {key_name}."
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload/")
def upload_file(file_path: str = Form(...), client_id: Optional[str] = Form(None)):
    bucket_name = "your-bucket-name"
    return {"message": upload_file_with_transfer_manager(file_path, bucket_name, client_id)}



===============
from fastapi import FastAPI, File, UploadFile, Form
from fastapi.responses import JSONResponse
import boto3
from botocore.exceptions import BotoCoreError, ClientError

app = FastAPI()

@app.post("/multipart-upload/")
async def multipart_upload(
    file: UploadFile = File(...),
    bucket_name: str = Form(...),
    key: str = Form(...),
    access_key: str = Form(...),
    secret_key: str = Form(...),
    endpoint_url: str = Form(...),
):
    """
    Multipart upload with internal chunking using UploadFile.
    
    Parameters:
    - file: File to upload (from UploadFile).
    - bucket_name: Target bucket in the storage service.
    - key: Destination path or key in the bucket.
    - access_key: Access key for authentication.
    - secret_key: Secret key for authentication.
    - endpoint_url: Endpoint URL for the S3-compatible service.
    """
    try:
        # Initialize the S3 client
        s3_client = boto3.client(
            "s3",
            aws_access_key_id=access_key,
            aws_secret_access_key=secret_key,
            endpoint_url=endpoint_url,
        )

        # Start multipart upload
        multipart_upload = s3_client.create_multipart_upload(Bucket=bucket_name, Key=key)
        upload_id = multipart_upload["UploadId"]

        # Chunk size (5 MB as required by S3-compatible services)
        chunk_size = 5 * 1024 * 1024
        parts = []
        part_number = 1

        # Read and upload the file in chunks
        while True:
            chunk = await file.read(chunk_size)
            if not chunk:
                break
            response = s3_client.upload_part(
                Bucket=bucket_name,
                Key=key,
                PartNumber=part_number,
                UploadId=upload_id,
                Body=chunk,
            )
            parts.append({"ETag": response["ETag"], "PartNumber": part_number})
            part_number += 1

        # Complete the multipart upload
        s3_client.complete_multipart_upload(
            Bucket=bucket_name,
            Key=key,
            MultipartUpload={"Parts": parts},
            UploadId=upload_id,
        )

        return {"message": "File uploaded successfully", "key": key}
    except (BotoCoreError, ClientError) as e:
        # Abort the multipart upload if something goes wrong
        try:
            s3_client.abort_multipart_upload(Bucket=bucket_name, Key=key, UploadId=upload_id)
        except Exception:
            pass  # Silent abort failure
        return JSONResponse(status_code=500, content={"error": str(e)})


====================
from pyspark.sql import SparkSession

# Initialize SparkSession with Iceberg configurations
spark = SparkSession.builder \
    .appName("IcebergExample") \
    .config("spark.sql.catalog.my_catalog", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.my_catalog.type", "hadoop") \
    .config("spark.sql.catalog.my_catalog.warehouse", "hdfs://namenode:8020/warehouse/path") \
    .getOrCreate()

# Read an Iceberg table
iceberg_table = spark.read.format("iceberg").load("my_catalog.database_name.table_name")

# Show data from the Iceberg table
iceberg_table.show()

# Create a new Iceberg table
spark.sql("""
CREATE TABLE my_catalog.database_name.new_table_name (
    id INT,
    name STRING,
    age INT
) USING iceberg
""")

# Insert data into the new Iceberg table
data = [(1, "Alice", 30), (2, "Bob", 25)]
df = spark.createDataFrame(data, schema=["id", "name", "age"])

# Write data to the Iceberg table
df.write.format("iceberg").mode("append").save("my_catalog.database_name.new_table_name")

# Verify the data in the new table
spark.sql("SELECT * FROM my_catalog.database_name.new_table_name").show()



==========÷=======
import os
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import lit, col
from pyspark.sql import DataFrame

# Initialize Spark session
spark = SparkSession.builder \
    .appName("Global RowOrder") \
    .enableHiveSupport() \
    .getOrCreate()

# Path to the directory containing Excel files
parent_dir = "/path/to/library/folders"

# Function to process each file and add local RowOrder
def process_excel_with_local_roworder(file_path, library_id, file_index, cumulative_offset):
    # Read Excel file into pandas
    pandas_df = pd.read_excel(file_path)

    # Add local RowOrder (starting from 0 for each file)
    pandas_df["LocalRowOrder"] = range(len(pandas_df))

    # Add a cumulative offset to make RowOrder globally unique
    pandas_df["RowOrder"] = pandas_df["LocalRowOrder"] + cumulative_offset

    # Add LibraryID for tracking
    pandas_df["LibraryID"] = library_id

    # Convert to PySpark DataFrame
    spark_df = spark.createDataFrame(pandas_df)

    return spark_df

# Initialize variables to track cumulative row offsets
all_spark_dfs = []
cumulative_offset = 0

# Process all files in the directory
for file_index, library_folder in enumerate(os.listdir(parent_dir)):
    folder_path = os.path.join(parent_dir, library_folder)
    if os.path.isdir(folder_path):
        for file_name in os.listdir(folder_path):
            if file_name.endswith((".xlsx", ".xls")):
                file_path = os.path.join(folder_path, file_name)

                # Read the file and process it with a global offset
                library_id = library_folder  # Use folder name as LibraryID
                spark_df = process_excel_with_local_roworder(file_path, library_id, file_index, cumulative_offset)

                # Update cumulative offset for the next file
                cumulative_offset += spark_df.count()

                # Collect the DataFrame
                all_spark_dfs.append(spark_df)

# Combine all DataFrames
final_spark_df = all_spark_dfs[0]
for df in all_spark_dfs[1:]:
    final_spark_df = final_spark_df.union(df)

# Sort by RowOrder to preserve the global order
final_spark_df = final_spark_df.orderBy("RowOrder")

# Write the final DataFrame to Hive
final_spark_df.write.mode("overwrite").saveAsTable("library_table")

print("Data successfully written to Hive table 'library_table'.")



==========÷=======
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import boto3
from botocore.exceptions import ClientError
from typing import Any, Optional, List
import io

app = FastAPI()

# Set up the Boto3 client for Ozone (Replace with actual credentials)
ozone_client = boto3.client(
    's3',
    endpoint_url="https://your-ozone-endpoint.com",  # Ozone Storage endpoint
    aws_access_key_id="your-access-key",
    aws_secret_access_key="your-secret-key"
)

### ✅ 1. Common Response Model
class BaseResponse(BaseModel):
    status: str
    data: Optional[Any] = None
    error: Optional[str] = None

### ✅ 2. Request Model for Creating Buckets
class BucketRequest(BaseModel):
    bucket_name: str

### ✅ 3. Health Check API
@app.get("/health-check", response_model=BaseResponse)
async def health_check():
    try:
        response = ozone_client.list_buckets()  # Checking if connection works
        return BaseResponse(status="success", data={"message": "Ozone connection is healthy!"})
    except ClientError as e:
        return BaseResponse(status="failure", error=str(e))

### ✅ 4. Create Bucket API
@app.post("/create-bucket", response_model=BaseResponse)
async def create_bucket(request: BucketRequest):
    bucket_name = request.bucket_name
    try:
        ozone_client.create_bucket(Bucket=bucket_name)
        bucket_info = {
            "name": bucket_name,
            "location": f"https://your-ozone-endpoint.com/{bucket_name}"
        }
        return BaseResponse(status="success", data=bucket_info)
    except ClientError as e:
        return BaseResponse(status="failure", error=str(e))

### ✅ 5. List Buckets API
@app.get("/list-buckets", response_model=BaseResponse)
async def list_buckets():
    try:
        response = ozone_client.list_buckets()
        bucket_names = [bucket['Name'] for bucket in response.get('Buckets', [])]
        return BaseResponse(status="success", data={"buckets": bucket_names})
    except ClientError as e:
        return BaseResponse(status="failure", error=str(e))

### ✅ 6. Get Bucket Info API
@app.get("/bucket-info", response_model=BaseResponse)
async def get_bucket_info(bucket_name: str):
    try:
        response = ozone_client.head_bucket(Bucket=bucket_name)  # Checking if bucket exists
        bucket_info = {
            "name": bucket_name,
            "location": f"https://your-ozone-endpoint.com/{bucket_name}",
            "exists": True
        }
        return BaseResponse(status="success", data=bucket_info)
    except ClientError as e:
        return BaseResponse(status="failure", error=str(e))

### ✅ 7. List Files API
@app.get("/list-files", response_model=BaseResponse)
async def list_files(bucket_name: str):
    try:
        response = ozone_client.list_objects_v2(Bucket=bucket_name)
        if 'Contents' not in response:
            return BaseResponse(status="success", data={"files": []})

        file_names = [obj['Key'] for obj in response['Contents']]
        return BaseResponse(status="success", data={"files": file_names})
    except ClientError as e:
        return BaseResponse(status="failure", error=str(e))

### ✅ 8. Download File API
@app.get("/download-file")
async def download_file(bucket_name: str, file_name: str):
    try:
        response = ozone_client.get_object(Bucket=bucket_name, Key=file_name)
        file_content = response['Body'].read()
        return StreamingResponse(io.BytesIO(file_content), media_type="application/octet-stream", headers={
            'Content-Disposition': f'attachment; filename="{file_name}"'
        })
    except ClientError as e:
        return HTTPException(status_code=400, detail=f"Error downloading file {file_name}: {e}")


=================
from fastapi import Query

@app.get("/file-info", response_model=BaseResponse)
async def get_file_info(
    bucket_name: str = Query(..., description="Bucket name"),
    file_name: str = Query(..., description="File name")
):
    try:
        # Get file metadata from Ozone
        response = ozone_client.head_object(Bucket=bucket_name, Key=file_name)

        file_info = {
            "file": file_name,
            "size": response.get("ContentLength", 0),
            "date_modified": response.get("LastModified").strftime("%Y-%m-%d %H:%M:%S") if "LastModified" in response else None,
            "type": response.get("ContentType", "unknown"),
            "status": "exists"
        }

        return BaseResponse(status="success", data=file_info)
    except ClientError as e:
        return BaseResponse(status="failure", error=str(e))



=========/============
@app.post("/rename-file", response_model=BaseResponse)
async def rename_file(
    bucket_name: str,
    old_file_name: str,
    new_file_name: str
):
    try:
        # Copy the file to new location
        copy_source = {"Bucket": bucket_name, "Key": old_file_name}
        ozone_client.copy_object(
            Bucket=bucket_name,
            CopySource=copy_source,
            Key=new_file_name
        )

        # Delete the old file
        ozone_client.delete_object(Bucket=bucket_name, Key=old_file_name)

        return BaseResponse(status="success", data={"message": f"File renamed from {old_file_name} to {new_file_name}"})
    except ClientError as e:
        return BaseResponse(status="failure", error=str(e))


==============
@app.delete("/delete-file", response_model=BaseResponse)
async def delete_file(
    bucket_name: str = Query(..., description="Bucket name"),
    file_name: str = Query(..., description="File to delete")
):
    try:
        ozone_client.delete_object(Bucket=bucket_name, Key=file_name)
        return BaseResponse(status="success", data={"message": f"File {file_name} deleted successfully"})
    except ClientError as e:
        return BaseResponse(status="failure", error=str(e))

============

@app.delete("/delete-bucket", response_model=BaseResponse)
async def delete_bucket(bucket_name: str = Query(..., description="Bucket to delete")):
    try:
        # Check if bucket is empty
        response = ozone_client.list_objects_v2(Bucket=bucket_name)
        if "Contents" in response:
            return BaseResponse(status="failure", error="Bucket is not empty. Delete all files first.")

        # Delete the bucket
        ozone_client.delete_bucket(Bucket=bucket_name)
        return BaseResponse(status="success", data={"message": f"Bucket {bucket_name} deleted successfully"})
    except ClientError as e:
        return BaseResponse(status="failure", error=str(e))

===========