# Extract the first three rows
header_rows = df.limit(3).collect()

# Combine the rows into column headers
new_columns = ["_".join([str(header_rows[j][i]) for j in range(3)]) for i in range(len(header_rows[0]))]

# Select the remaining rows and assign the new columns
# Skip the first 3 rows using filter with row_number
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Adding a row number column to identify and skip the first 3 rows
df_with_row_num = df.withColumn("row_num", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))

# Filtering out the first 3 rows
df_filtered = df_with_row_num.filter(F.col("row_num") > 3).drop("row_num")

# Applying new column names to the remaining DataFrame
for i, new_col in enumerate(new_columns):
    df_filtered = df_filtered.withColumnRenamed(df_filtered.columns[i], new_col)

# Display the DataFrame
df_filtered.show()
