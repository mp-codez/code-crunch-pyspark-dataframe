# Extract the first three rows
header_rows = df.limit(3).collect()

# Combine the rows into column headers
new_columns = ["_".join([str(header_rows[j][i]) for j in range(3)]) for i in range(len(header_rows[0]))]

# Select the remaining rows and assign the new columns
# Skip the first 3 rows using filter with row_number
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Adding a row number column to identify and skip the first 3 rows
df_with_row_num = df.withColumn("row_num", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))

# Filtering out the first 3 rows
df_filtered = df_with_row_num.filter(F.col("row_num") > 3).drop("row_num")

# Applying new column names to the remaining DataFrame
for i, new_col in enumerate(new_columns):
    df_filtered = df_filtered.withColumnRenamed(df_filtered.columns[i], new_col)

# Display the DataFrame
df_filtered.show()


≈=============/===============
# Step 1: Extract the first 2 rows to use as header rows
header_rows = df.limit(2).collect()

# Step 2: Set up new column names combining both rows
column_names = [f"{str(header_rows[0][i])} ({str(header_rows[1][i])})" for i in range(len(header_rows[0]))]

# Step 3: Skip the first 2 rows and keep the remaining data
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Add row number to identify and filter out the first 2 rows
df_with_row_num = df.withColumn("row_num", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))

# Filter out the first 2 rows, keeping only the data rows
df_filtered = df_with_row_num.filter(F.col("row_num") > 2).drop("row_num")

# Step 4: Rename columns with the new headers
df_final = df_filtered.toDF(*column_names)

# Display the final DataFrame
df_final.show()


№###№@###########
from pyspark.sql.types import StructType, StructField, StringType
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Step 1: Extract the first 2 rows
header_rows = df.limit(2).collect()

# Step 2: Define column names and units from the extracted rows
column_names = [str(header_rows[0][i]) for i in range(len(header_rows[0]))]
unit_types = [str(header_rows[1][i]) for i in range(len(header_rows[1]))]

# Step 3: Create StructType schema, leaving units blank for the first 5 columns
schema = StructType([
    StructField(f"{col_name}" if i < 5 else f"{col_name} ({unit})", StringType(), True)
    for i, (col_name, unit) in enumerate(zip(column_names, unit_types))
])

# Step 4: Skip the first 2 rows and apply the new schema
df_with_row_num = df.withColumn("row_num", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))
df_filtered = df_with_row_num.filter(F.col("row_num") > 2).drop("row_num")

# Apply the schema
df_final = spark.createDataFrame(df_filtered.rdd, schema)

# Display the final DataFrame
df_final.show()



№########
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Initialize Spark session
spark = SparkSession.builder.appName("PartialPressureCalculation").getOrCreate()

def partial_pressure(df_s, df_xy, lst):
    # Select only the relevant columns
    df_mini = df_xy.select("LibraryID", "ExperimentID", "Position", "Pre-InjectPressure")

    # Get the first and last occurrences by grouping with a window function
    window_spec = Window.partitionBy("LibraryID", "ExperimentID", "Position").orderBy("some_order_column")  # Replace 'some_order_column' with a timestamp or other order indicator if available.

    # First occurrence
    first_occurrences = df_mini.withColumn("first_occurrence", F.first("Pre-InjectPressure").over(window_spec))

    # Last occurrence
    last_occurrences = df_mini.withColumn("last_occurrence", F.last("Pre-InjectPressure").over(window_spec))

    # Calculate the difference between the first and last occurrences
    result_df = first_occurrences.join(last_occurrences, on=["LibraryID", "ExperimentID", "Position"])
    result_df = result_df.withColumn(lst[92], F.col("last_occurrence") - F.col("first_occurrence"))

    # Remove duplicates, keeping only the last occurrence of each group
    result_df = result_df.dropDuplicates(["LibraryID", "ExperimentID", "Position"])

    # Sort the DataFrame by specified columns
    result_df = result_df.orderBy("LibraryID", "ExperimentID", "Position")

    # Create a unique identifier by concatenating columns
    result_df = result_df.withColumn("Unique Experiment ID (PPR = LibID+ExpID)", 
                                     F.concat_ws("-", F.col("LibraryID"), F.col("ExperimentID")))

    # Drop unnecessary columns
    result_df = result_df.drop("LibraryID", "ExperimentID", "Pre-InjectPressure")

    # Assuming df_s has the relevant columns to merge with, we join it on the unique experiment ID
    merged_df = df_s.join(result_df, df_s["Unique Experiment ID (PPR = LibID+ExpID)"] == result_df["Unique Experiment ID (PPR = LibID+ExpID)"], "left")

    # Drop the 'Position' column from the final DataFrame, if needed
    merged_df = merged_df.drop("Position")

    return merged_df

# Use the function with your DataFrames
# result_df = partial_pressure(df_s, df_xy, lst)


______<&><<<<<<<
from pyspark.sql.functions import col, sum as spark_sum, when
from pyspark.sql import DataFrame

# Assuming liquid_columns is a list of column names
liquid_columns = ['col1', 'col2', 'col3']  # Replace with your actual column names

# Convert columns to numeric, handle errors by replacing invalid values with null, and sum them
df_s = df_s.select(
    *df_s.columns,
    spark_sum(
        *[when(col(c).cast('double').isNotNull(), col(c).cast('double')).otherwise(0) for c in liquid_columns]
    ).alias('sum_column')
)

df_s.show()



≈=====================
from pyspark.sql import SparkSession
from pyspark.sql.functions import coalesce

# Initialize Spark session
spark = SparkSession.builder.appName("Update Columns Example").getOrCreate()

# Sample DataFrames (Assume `df1` and `df2` are defined similarly in your setup)
data1 = [
    (1, "A", 100, "val1", "val2", "val3", "val4"),
    (2, "B", 200, "val5", "val6", "val7", "val8")
]
columns1 = ["col1", "col2", "col3", "fill_col1", "fill_col2", "fill_col3", "fill_col4"]
df1 = spark.createDataFrame(data1, columns1)

data2 = [
    (1, "A", 100, None, None, None, None),
    (2, "B", 200, None, None, None, None),
    (3, "C", 300, "existing1", "existing2", "existing3", "existing4")
]
columns2 = ["col1", "col2", "col3", "fill_col1", "fill_col2", "fill_col3", "fill_col4"]
df2 = spark.createDataFrame(data2, columns2)

# Perform a left join on the common columns
joined_df = df2.alias("df2").join(
    df1.alias("df1"),
    on=(df2["col1"] == df1["col1"]) & 
       (df2["col2"] == df1["col2"]) & 
       (df2["col3"] == df1["col3"]),
    how="left"
)

# Select all columns from df2, but update the target columns using coalesce
# Coalesce will pick values from df1 when they exist, otherwise df2 values are retained
updated_df2 = joined_df.select(
    df2["col1"], df2["col2"], df2["col3"],  # The common columns
    coalesce(df1["fill_col1"], df2["fill_col1"]).alias("fill_col1"),
    coalesce(df1["fill_col2"], df2["fill_col2"]).alias("fill_col2"),
    coalesce(df1["fill_col3"], df2["fill_col3"]).alias("fill_col3"),
    coalesce(df1["fill_col4"], df2["fill_col4"]).alias("fill_col4")
)

# Show the updated DataFrame
updated_df2.show()


≈======================================
import boto3
import math
from boto3.s3.transfer import TransferConfig

def multipart_upload(bucket_name, file_path, object_name):
    s3_client = boto3.client('s3')

    # Define the part size (e.g., 5 MB)
    part_size = 5 * 1024 * 1024

    # Get the file size
    file_size = os.path.getsize(file_path)

    # Create a transfer configuration
    config = TransferConfig(multipart_chunksize=part_size, use_threads=True)

    # Upload the file using multipart upload
    with open(file_path, 'rb') as file:
        s3_client.upload_fileobj(
            file,
            bucket_name,
            object_name,
            Config=config
        )
    print(f"{file_path} uploaded to S3 bucket {bucket_name} as {object_name}.")

# Example usage
multipart_upload(
    bucket_name='your-bucket-name',
    file_path='/path/to/large/file.txt',
    object_name='large-file.txt'
)

============≈==============//=========
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

def send_email(sender_email, receiver_email, subject, body, smtp_server, port, username=None, password=None):
    try:
        # Create a MIMEText email
        msg = MIMEMultipart()
        msg['From'] = sender_email
        msg['To'] = receiver_email
        msg['Subject'] = subject
        msg.attach(MIMEText(body, 'plain'))

        # Establish connection with the SMTP server
        with smtplib.SMTP(smtp_server, port) as server:
            server.starttls()  # Start TLS encryption
            if username and password:
                server.login(username, password)  # Authenticate with the server
            server.sendmail(sender_email, receiver_email, msg.as_string())  # Send the email
        
        print("Email sent successfully!")
    except Exception as e:
        print(f"Failed to send email: {e}")

# Example usage
send_email(
    sender_email="your_email@example.com",
    receiver_email="recipient_email@example.com",
    subject="Test Email",
    body="This is a test email sent from Python.",
    smtp_server="smtp.example.com",  # Replace with your SMTP server
    port=587,  # Use 465 for SSL or 587 for TLS
    username="your_email_username",  # Optional if server requires authentication
    password="your_email_password"   # Optional if server requires authentication
)


≈===============
import smtplib
import socket
import time
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

def is_server_up(host, port):
    """
    Check if the server is up by attempting a socket connection.
    """
    try:
        with socket.create_connection((host, port), timeout=5):
            return True
    except (socket.timeout, ConnectionError):
        return False

def send_email(sender_email, sender_password, recipient_email, subject, body):
    """
    Send an email notification using SMTP.
    """
    try:
        # Set up the email
        message = MIMEMultipart()
        message['From'] = sender_email
        message['To'] = recipient_email
        message['Subject'] = subject
        message.attach(MIMEText(body, 'plain'))
        
        # Connect to the SMTP server and send the email
        with smtplib.SMTP('smtp.gmail.com', 587) as server:
            server.starttls()
            server.login(sender_email, sender_password)
            server.send_message(message)
        print("Email sent successfully.")
    except Exception as e:
        print(f"Failed to send email: {e}")

def monitor_server(host, port, sender_email, sender_password, recipient_email):
    """
    Monitor server status every 5 minutes and send email if server is down.
    """
    while True:
        if not is_server_up(host, port):
            subject = f"Alert: Server {host} is Down"
            body = f"The server at {host} on port {port} is not responding."
            print(body)  # Log the issue
            send_email(sender_email, sender_password, recipient_email, subject, body)
        else:
            print(f"The server {host} is up and running.")  # Log the status
        time.sleep(300)  # Wait for 5 minutes before the next check

if __name__ == "__main__":
    # Server details
    host = "your_server_ip_or_hostname"
    port = 80  # Change to the port you want to monitor

    # Email details
    sender_email = "your_email@gmail.com"
    sender_password = "your_email_password"  # Use an app password if needed
    recipient_email = "recipient_email@gmail.com"

    monitor_server(host, port, sender_email, sender_password, recipient_email)



==========
from fastapi import FastAPI, Form, HTTPException
from pydantic import BaseModel
import os
from typing import Optional
import boto3
from boto3.s3.transfer import TransferConfig
from datetime import datetime
import uuid

app = FastAPI()

def upload_file_with_transfer_manager(file_path: str, bucket_name: str, client_id: Optional[str] = None):
    """
    Upload a file using TransferManager with a unique key for each client.
    """
    client = boto3.client(
        's3',
        endpoint_url='http://ozone-server:9878',  # Replace with your Ozone endpoint
        aws_access_key_id='your_access_key',
        aws_secret_access_key='your_secret_key'
    )

    unique_id = uuid.uuid4()
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    key_name = f"{client_id}_{timestamp}_{unique_id}_{os.path.basename(file_path)}" if client_id else f"{timestamp}_{unique_id}_{os.path.basename(file_path)}"

    config = TransferConfig(
        multipart_threshold=5 * 1024 * 1024,
        multipart_chunksize=5 * 1024 * 1024,
        use_threads=True
    )

    try:
        client.upload_file(
            Filename=file_path,
            Bucket=bucket_name,
            Key=key_name,
            Config=config
        )
        return f"File {file_path} uploaded successfully as {key_name}."
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload/")
def upload_file(file_path: str = Form(...), client_id: Optional[str] = Form(None)):
    bucket_name = "your-bucket-name"
    return {"message": upload_file_with_transfer_manager(file_path, bucket_name, client_id)}