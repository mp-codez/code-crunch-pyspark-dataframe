# Extract the first three rows
header_rows = df.limit(3).collect()

# Combine the rows into column headers
new_columns = ["_".join([str(header_rows[j][i]) for j in range(3)]) for i in range(len(header_rows[0]))]

# Select the remaining rows and assign the new columns
# Skip the first 3 rows using filter with row_number
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Adding a row number column to identify and skip the first 3 rows
df_with_row_num = df.withColumn("row_num", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))

# Filtering out the first 3 rows
df_filtered = df_with_row_num.filter(F.col("row_num") > 3).drop("row_num")

# Applying new column names to the remaining DataFrame
for i, new_col in enumerate(new_columns):
    df_filtered = df_filtered.withColumnRenamed(df_filtered.columns[i], new_col)

# Display the DataFrame
df_filtered.show()


â‰ˆ=============/===============
# Step 1: Extract the first 2 rows to use as header rows
header_rows = df.limit(2).collect()

# Step 2: Set up new column names combining both rows
column_names = [f"{str(header_rows[0][i])} ({str(header_rows[1][i])})" for i in range(len(header_rows[0]))]

# Step 3: Skip the first 2 rows and keep the remaining data
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Add row number to identify and filter out the first 2 rows
df_with_row_num = df.withColumn("row_num", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))

# Filter out the first 2 rows, keeping only the data rows
df_filtered = df_with_row_num.filter(F.col("row_num") > 2).drop("row_num")

# Step 4: Rename columns with the new headers
df_final = df_filtered.toDF(*column_names)

# Display the final DataFrame
df_final.show()